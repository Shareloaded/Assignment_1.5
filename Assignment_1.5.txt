1. Hadoop in layman's term :
Hadoop is an open source(free to use by anyone) apache framework written in java.
Hadoop can be thought of as a set of open source programs and procedures  which anyone can use as the "backbone" of their big data operations.
Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data. 
Its distributed file system facilitates rapid data transfer rates among nodes and allows the system to continue operating in case of a node failure. 
This approach lowers the risk of catastrophic system failure and unexpected data loss, even if a significant number of nodes become inoperative.

2. Components of Hadoop framework :
  The 3 core components of the Apache Software Foundation’s Hadoop framework are:

1. MapReduce – A software programming model for processing large sets of data in parallel
2. HDFS – The Java-based distributed file system that can store all kinds of data without prior organization.
3. YARN – A resource management framework for scheduling and handling resource requests from distributed applications.

HDFS is the storage sheath of Hadoop. It takes care of storing data of petabyte scale.
Saturation makes it necessary to think laterally and marches towards scaling.
As, and when data, grows vigorously, it is constantly challenging the human perception of building and stacking data storage in the “vertical” form (i.e. accommodating data growth only on a single machine, the concept of “scaling up” was facing chronic saturation.)
Hadoop may also contain
1.Zookeeper – This is basically a centralized system that maintains –
   Configuration information
   Naming information
   Synchronization information
Besides these, Zookeeper is also responsible for group services and is utilized by HBASE. It also comes to use for MapReduce programs.
2.Solr/Lucene – This is  a search engine. 

3.
1. No signs of slowing down.
 More people have access to mobile devices that sense and acquire information through the likes of cameras, microphones, etc. 
Consider this, since 2012, about 2.5 exabytes of data are created daily! That said, the Big Data revolution will continue to grow.

2. Everyone uses Big Data.
 Big Data is everywhere, from politics to health care to even sports. 
Big Data analysis played a crucial role in President Obama’s successful reelection in 2012. 
Healthcare organizations use it to provide more personalized prescriptions, predictive analysis, and many other services.
 And sports-wise, more teams are using Big Data analysis to scout for athletes who best fit their needs.

3. Information Managers in Demand.
Someone has to be able to implement, run, and manage the software used to analyze Big Data. 
So, in conjunction with the rise of Big Data, the demand for information management specialists has increased.
 Knowing how to use Big Data technology/software can make you highly desirable in a number of industries, 
even more so if you are able to break down that data and make it more streamlined. 
Other desired Big Data skills include data mining, information warehousing, and ETL (Extract. Transform. Load).

4. Software Options Galore. 
Just how big is Big Data? Major players such as Microsoft, IBM, Oracle have spent billions investing in data analytics and management software. 
There are a lot of solutions out there. There are also open source options such as Apache Hadoop, meaning the cost of entry doesn’t have to be outrageous.

5. You’ll learn other tech.
Want to help companies really leverage Big Data? You’ll need to be able to utilize cloud-based services. 
They not only can provide the storage needed for large amounts of data, but the power needed to manage and analyze all of the data.
 Also, because Big Data is more than just numbers