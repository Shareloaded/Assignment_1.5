1. Hadoop in layman's term :
Hadoop is an open source(free to use by anyone) apache framework written in java.
Hadoop can be thought of as a set of open source programs and procedures  which anyone can use as the "backbone" of their big data operations.
Hadoop makes it possible to run applications on systems with thousands of commodity hardware nodes, and to handle thousands of terabytes of data. 
Its distributed file system facilitates rapid data transfer rates among nodes and allows the system to continue operating in case of a node failure. 
This approach lowers the risk of catastrophic system failure and unexpected data loss, even if a significant number of nodes become inoperative.

The 4 Modules of Hadoop
Hadoop is made up of "modules", each of which carries out a particular task essential for a computer system designed for big data analytics.
1. Distributed File-System
The most important two are the Distributed File System, which allows data to be stored in an easily accessible format, across a large number of linked storage devices, and the MapReduce - which provides the basic tools for poking around in the data.
2. MapReduce
MapReduce is named after the two basic operations this module carries out - reading data from the database, putting it into a format suitable for analysis (map), and performing mathematical operations i.e counting the number of males aged 30+ in a customer database (reduce).
3. Hadoop Common
The other module is Hadoop Common, which provides the tools (in Java) needed for the user's computer systems (Windows, Unix or whatever) to read data stored under the Hadoop file system.
4. YARN
The final module is YARN, which manages resources of the systems storing the data and running the analysis.
Various other procedures, libraries or features have come to be considered part of the Hadoop "framework" over recent years, 
but Hadoop Distributed File System, Hadoop MapReduce, Hadoop Common and Hadoop YARN are the principle four.





2. Components of Hadoop framework :
   Hadoop Distributed File System –Abbreviated as HDFS, it is primarily a file system similar to many of the already existing ones. However, it is also a virtual file system.
                                   There is one notable difference with other popular file systems, which is, when we move a file in HDFS, it is automatically split into smaller files. These smaller files are then replicated on a minimum of three different servers, 
                                   so that they can be used as an alternative to unforeseen circumstances.

   Hadoop MapReduce – MapReduce is mainly the programming aspect of Hadoop that allows processing of large volumes of data.
   HBASE – HBASE happens to be a layer that sits atop the HDFS and has been developed by means of the Java programming language. HBASE primarily has the following aspects –
           Non relational
           Highly scalable
           Fault tolerance
   Zookeeper – This is basically a centralized system that maintains –
               Configuration information
               Naming information
               Synchronization information
   Solr/Lucene – This is nothing but a search engine. Its libraries are developed by Apache and required over 10 years to be developed in its present robust form.
   Programming Languages – There are basically two programming languages that are identified as original Hadoop programming languages,
                             Hive
                             PIG 
   Besides these, there are a few other programming languages that can be used for writing programs, namely C, JAQL and Java. We can also make direct usage of SQL for interaction with the database, although that requires the use of standard JDBC or ODBC drivers.










3. Reasons to learn Big data technologies :
   Personal Reasons:
     1.New Job.
     2.High salary.
     3.High demand.
     4.Skill Development.
   
   Business Reason: A new style of IT emerging every 60 seconds,Oracle license cost increases every year.
   
   Others:
  Demand for Big Data skills is extremely high, and being able to prove your expertise is
critical:
  64% of IT hiring managers rate skilled big data knowledge as having extremely high or
high value when rating expertise of candidates, based on a survey by CompTIA
  According to Forbes, the median advertised salary for professionals with Big Data
expertise is $124,000 a year
   IBM, Cisco, and Oracle together advertised 26,488 open positions that required Big
Data expertise in the last twelve months